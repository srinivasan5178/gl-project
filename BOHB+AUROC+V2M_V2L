# ============================================================
# FINAL STABLE PIPELINE - SAFE VERSION
# Domain-aware Oversampling + CNN + CatBoost + ROC/AUROC + Time  ( Final Working code)
# ============================================================

import os
import shutil
import random
import time
import json
import datetime # Import the datetime module
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# -------------------------
# TensorFlow / Keras
# -------------------------
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import DenseNet201, EfficientNetV2M , EfficientNetV2L # Added EfficientNetV2M
from tensorflow.keras.regularizers import l2

# -------------------------
# Metrics
# -------------------------
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score # Added roc_auc_score
from sklearn.preprocessing import label_binarize

# -------------------------
# CatBoost
# -------------------------
from catboost import CatBoostClassifier

# -------------------------
# BOHB / Pyro
# -------------------------
from hpbandster.core.worker import Worker
import Pyro4

# ============================================================
# CONFIG
# ============================================================
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

original_dataset_dir = "/content/Project_Sample"
balanced_temp_dir   = "/content/Project_Sample_balanced"
split_dataset_dir   = "/content/Project_Sample_split"

outputs_root = "./runs"
plots_dir = os.path.join(outputs_root, "plots")
os.makedirs(plots_dir, exist_ok=True)

RATIO = (0.7, 0.15, 0.15)
BATCH_SIZE = 32
EPOCHS = 15

CLASS_NAMES_COLON = ['colon_aca', 'colon_n']
CLASS_NAMES_LUNG = ['lung_aca', 'lung_n', 'lung_scc']
GLOBAL_CLASS_NAMES = CLASS_NAMES_COLON + CLASS_NAMES_LUNG # Total 5 classes
N_CLASSES = len(GLOBAL_CLASS_NAMES)

# ============================================================
# UTILITIES
# ============================================================
def clean_folder(path):
    if os.path.exists(path):
        shutil.rmtree(path)
    os.makedirs(path, exist_ok=True)

def save_json(d, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w') as f:
        json.dump(d, f, indent=2)

# ============================================================
# DOMAIN-AWARE OVERSAMPLING
# ============================================================
def oversample_all_classes_flat(input_root, balanced_root, seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    clean_folder(balanced_root)

    files_per_class = {}
    # Correctly identify actual class folders, even if nested under domain folders
    for domain_or_class_dir in os.listdir(input_root):
        current_path = os.path.join(input_root, domain_or_class_dir)
        if not os.path.isdir(current_path) or domain_or_class_dir.startswith('.'):
            continue

        # Check if this is a domain folder (e.g., 'colon', 'lung') or directly a class folder (e.g., 'colon_aca')
        sub_dirs = [d for d in os.listdir(current_path) if os.path.isdir(os.path.join(current_path, d))]
        if any(sc in GLOBAL_CLASS_NAMES for sc in sub_dirs): # Likely a domain folder
            for cls_name in sub_dirs:
                cls_path = os.path.join(current_path, cls_name)
                images = [
                    os.path.join(cls_path, f)
                    for f in os.listdir(cls_path)
                    if f.lower().endswith(('.png','.jpg','.jpeg'))
                ]
                files_per_class[cls_name] = images
                print(f"{cls_name}: {len(images)} images")
        else: # Assumed to be a flat class folder
            cls_name = domain_or_class_dir
            images = [
                os.path.join(current_path, f)
                for f in os.listdir(current_path)
                if f.lower().endswith(('.png','.jpg','.jpeg'))
            ]
            files_per_class[cls_name] = images
            print(f"{cls_name}: {len(images)} images")


    non_empty = {k:v for k,v in files_per_class.items() if len(v) > 0}
    if not non_empty:
        raise ValueError("❌ No images found in dataset")

    max_count = max(len(v) for v in non_empty.values())
    print(f"\nOversampling all classes to {max_count} samples each\n")

    for cls, files in files_per_class.items():
        if len(files) == 0:
            continue
        out_dir = os.path.join(balanced_root, cls)
        os.makedirs(out_dir, exist_ok=True)
        replicated = (files * (max_count // len(files) + 1))[:max_count]
        for i, src in enumerate(replicated):
            ext = os.path.splitext(src)[1]
            shutil.copy2(src, os.path.join(out_dir, f"{cls}_{i}{ext}"))

    print("✅ Oversampling completed")

# ============================================================
# SPLIT DATASET
# ============================================================
def split_balanced_to_train_val_test(balanced_root, output_root, ratio=RATIO):
    clean_folder(output_root)
    for cls in sorted(os.listdir(balanced_root)):
        cls_path = os.path.join(balanced_root, cls)
        if not os.path.isdir(cls_path): # Skip non-directories
            continue

        files = os.listdir(cls_path)
        random.shuffle(files)

        n = len(files)
        n_train = int(n * ratio[0])
        n_val = int(n * ratio[1])

        splits = {
            "train": files[:n_train],
            "val": files[n_train:n_train+n_val],
            "test": files[n_train+n_val:]
        }

        for split, flist in splits.items():
            out = os.path.join(output_root, split, cls)
            os.makedirs(out, exist_ok=True)
            for f in flist:
                shutil.copy2(os.path.join(balanced_root, cls, f), out)

# ============================================================
# DATA GENERATOR
# ============================================================
def create_generator(path, classes, shuffle=True):
    gen = ImageDataGenerator(rescale=1./255)
    return gen.flow_from_directory(
        path,
        target_size=(224,224),
        batch_size=BATCH_SIZE,
        class_mode='sparse',
        classes=classes, # Pass specific classes
        shuffle=shuffle
    )

# ============================================================
# CNN MODEL
# ============================================================
def build_cnn(domain_type, lr, dropout, weight_decay):
    num_output_classes = len(CLASS_NAMES_COLON) if domain_type == 'colon' else len(CLASS_NAMES_LUNG)

    # Use DenseNet201 for colon and EfficientNetV2M for lung
   # base_model_cls = DenseNet201 if domain_type == 'colon' else EfficientNetV2M

    # Use DenseNet201 for colon and EfficientNetV2L for lung
    base_model_cls = DenseNet201 if domain_type == 'colon' else EfficientNetV2L

    base = base_model_cls(weights='imagenet', include_top=False, input_shape=(224,224,3))
    base.trainable = False

    x_input = base.input # Explicitly define the input tensor
    x = GlobalAveragePooling2D()(base.output)
    x = Dense(256, activation='relu', name="penultimate", kernel_regularizer=l2(weight_decay))(x)
    x = Dropout(dropout)(x) # This is the line that caused the error
    out = Dense(num_output_classes, activation='softmax')(x)

    model = Model(base.input, out)
    model.compile(
        optimizer=Adam(lr),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# ============================================================
# FEATURE EXTRACTION
# ============================================================
def extract_features(model, generator, steps):
    feat_model = Model(model.input, model.get_layer("penultimate").output)
    feats, labs = [], []

    # Iterate through the generator for the specified number of steps
    for i in range(steps):
        x, y = generator[i] # Access generator by index
        feats.append(feat_model.predict(x, verbose=0))
        labs.append(y)

    return np.vstack(feats), np.hstack(labs)

# ============================================================
# CONFUSION MATRIX
# ============================================================
def save_confusion_matrix(y_true, y_pred, split_name):
    # Ensure y_pred is 1D for concatenation if it comes as (N, 1)
    if y_pred.ndim > 1:
        y_pred = y_pred.flatten()

    # Filter out labels that are not present in either y_true or y_pred but are in GLOBAL_CLASS_NAMES
    # This ensures display_labels matches existing labels
    present_labels = np.unique(np.concatenate([y_true, y_pred])).astype(int)
    display_labels_names = [GLOBAL_CLASS_NAMES[i] for i in present_labels]

    cm = confusion_matrix(y_true, y_pred, labels=present_labels)
    disp = ConfusionMatrixDisplay(cm, display_labels=display_labels_names)
    fig, ax = plt.subplots(figsize=(8,6))
    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')
    plt.title(f"{split_name.capitalize()} Confusion Matrix")
    plt.tight_layout()
    path = os.path.join(plots_dir, f"{split_name}_confusion_matrix.png")
    plt.savefig(path)
    plt.close(fig)
    print(f"Saved {split_name} confusion matrix to {path}")
    return cm

# ============================================================
# ROC / AUROC
# ============================================================
def compute_macro_roc(y_true, y_prob, split):
    # Ensure y_true contains all classes that might be in y_prob for binarization
    # Map y_true to a 0 to N-1 range where N is the total number of global classes
    y_true_mapped = np.array(y_true, dtype=int)

    # Binarize labels considering all possible global classes
    y_bin = label_binarize(y_true_mapped, classes=list(range(N_CLASSES)))

    # Adjust y_prob if it contains probabilities for fewer classes than N_CLASSES
    # This happens if a domain-specific model is used and its output is directly passed here
    if y_prob.shape[1] < N_CLASSES:
        # Create a new y_prob array with N_CLASSES columns, filling missing with 0s
        full_y_prob = np.zeros((y_prob.shape[0], N_CLASSES))
        # Assuming colon classes are 0,1 and lung classes are 2,3,4
        # This part needs careful handling based on how y_prob is generated.
        # If y_prob is from the final combined CatBoost model, it already has N_CLASSES.
        # If this is for intermediate steps, it would require more context.
        # For the final CatBoost predictions, y_prob should already have N_CLASSES columns.
        pass # No change needed if y_prob is already 5-column from combined CatBoost

    fpr, tpr = {}, {}
    for i in range(N_CLASSES):
        # Only compute ROC for classes actually present in y_true_mapped
        if i in np.unique(y_true_mapped):
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_prob[:, i])
        else:
            # If a class is not present, plot a diagonal line for its ROC
            fpr[i], tpr[i] = np.array([0., 1.]), np.array([0., 1.])

    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(N_CLASSES)]))
    mean_tpr = np.zeros_like(all_fpr)
    count_present_classes = 0
    for i in range(N_CLASSES):
        if i in np.unique(y_true_mapped):
            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
            count_present_classes += 1
    if count_present_classes > 0:
        mean_tpr /= count_present_classes
    else:
        mean_tpr = all_fpr # If no classes, default to random classifier line

    macro_auc = auc(all_fpr, mean_tpr)

    plt.figure()
    plt.plot(all_fpr, mean_tpr, label=f"Macro AUC = {macro_auc:.4f}")
    plt.plot([0,1],[0,1],'k--')
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title(f"{split} Macro ROC")
    plt.legend()
    plt.savefig(os.path.join(plots_dir, f"{split}_macro_roc.png"))
    plt.close()
    return macro_auc

# ============================================================
# TIME TRACKER
# ============================================================
class TimeTracker:
    def __init__(self):
        self.start = time.time()
    def stop(self):
        return time.time() - self.start

# ============================================================
# BOHB WORKER
# ============================================================
class CNNCatBoostWorker(Worker):
    def compute(self, config, budget, *args, **kwargs):
        timer = TimeTracker()

        # Create domain-specific generators from the split_dataset_dir
        train_colon_gen = create_generator(os.path.join(split_dataset_dir, 'train'), CLASS_NAMES_COLON)
        train_lung_gen = create_generator(os.path.join(split_dataset_dir, 'train'), CLASS_NAMES_LUNG)
        val_colon_gen = create_generator(os.path.join(split_dataset_dir, 'val'), CLASS_NAMES_COLON, shuffle=False)
        val_lung_gen = create_generator(os.path.join(split_dataset_dir, 'val'), CLASS_NAMES_LUNG, shuffle=False)
        test_colon_gen = create_generator(os.path.join(split_dataset_dir, 'test'), CLASS_NAMES_COLON, shuffle=False)
        test_lung_gen = create_generator(os.path.join(split_dataset_dir, 'test'), CLASS_NAMES_LUNG, shuffle=False)

        # Build and train colon CNN
        cnn_colon = build_cnn('colon', config['learning_rate'], config['dropout_rate'], config['weight_decay'])
        cnn_colon.fit(train_colon_gen, epochs=EPOCHS, # Use EPOCHS from config
                              validation_data=val_colon_gen,
                              steps_per_epoch=len(train_colon_gen),
                              validation_steps=len(val_colon_gen),
                              verbose=0)

        # Build and train lung CNN
        cnn_lung = build_cnn('lung', config['learning_rate'], config['dropout_rate'], config['weight_decay'])
        cnn_lung.fit(train_lung_gen, epochs=EPOCHS, # Use EPOCHS from config
                             validation_data=val_lung_gen,
                             steps_per_epoch=len(train_lung_gen),
                             validation_steps=len(val_lung_gen),
                             verbose=0)

        # Extract features from both CNNs for the full dataset splits
        t_feat_start = time.time()
        X_train_colon_feats, y_train_colon_labels_raw = extract_features(cnn_colon, train_colon_gen, steps=len(train_colon_gen))
        X_train_lung_feats, y_train_lung_labels_raw = extract_features(cnn_lung, train_lung_gen, steps=len(train_lung_gen))

        X_val_colon_feats, y_val_colon_labels_raw = extract_features(cnn_colon, val_colon_gen, steps=len(val_colon_gen))
        X_val_lung_feats, y_val_lung_labels_raw = extract_features(cnn_lung, val_lung_gen, steps=len(val_lung_gen))

        X_test_colon_feats, y_test_colon_labels_raw = extract_features(cnn_colon, test_colon_gen, steps=len(test_colon_gen))
        X_test_lung_feats, y_test_lung_labels_raw = extract_features(cnn_lung, test_lung_gen, steps=len(test_lung_gen))

        t_feat = time.time() - t_feat_start

        # Remap lung labels and combine features and labels for CatBoost
        y_train_colon_mapped = y_train_colon_labels_raw
        y_train_lung_mapped = y_train_lung_labels_raw + len(CLASS_NAMES_COLON)

        y_val_colon_mapped = y_val_colon_labels_raw
        y_val_lung_mapped = y_val_lung_labels_raw + len(CLASS_NAMES_COLON)

        y_test_colon_mapped = y_test_colon_labels_raw
        y_test_lung_mapped = y_test_lung_labels_raw + len(CLASS_NAMES_COLON)

        # Combine features and labels
        X_train_combined = np.vstack([X_train_colon_feats, X_train_lung_feats])
        y_train_combined = np.hstack([y_train_colon_mapped, y_train_lung_mapped])

        X_val_combined = np.vstack([X_val_colon_feats, X_val_lung_feats])
        y_val_combined = np.hstack([y_val_colon_mapped, y_val_lung_mapped])

        X_test_combined = np.vstack([X_test_colon_feats, X_test_lung_feats])
        y_test_combined = np.hstack([y_test_colon_mapped, y_test_lung_mapped])

        # CatBoost training
        t_cb_start = time.time()
        cb = CatBoostClassifier(
            iterations=config['iterations'],
            depth=config['depth'],
            learning_rate=config['lr_catboost'],
            loss_function='MultiClass',
            verbose=0,
            random_seed=SEED # Ensure reproducibility
        )
        cb.fit(X_train_combined, y_train_combined, eval_set=(X_val_combined, y_val_combined), verbose=0)
        t_cb = time.time() - t_cb_start

        # Predictions and evaluation on all sets
        t_pred_start = time.time()
        train_probs_combined = cb.predict_proba(X_train_combined)
        train_preds_combined = cb.predict(X_train_combined).flatten() # Flatten output

        val_probs_combined = cb.predict_proba(X_val_combined)
        val_preds_combined = cb.predict(X_val_combined).flatten() # Flatten output

        test_probs_combined = cb.predict_proba(X_test_combined)
        test_preds_combined = cb.predict(X_test_combined).flatten() # Flatten output
        t_pred = time.time() - t_pred_start

        # Accuracies
        train_accuracy = accuracy_score(y_train_combined, train_preds_combined)
        val_accuracy = accuracy_score(y_val_combined, val_preds_combined)
        test_accuracy = accuracy_score(y_test_combined, test_preds_combined)

        # Macro AUCs
        train_macro_auc = compute_macro_roc(y_train_combined, train_probs_combined, "train")
        val_macro_auc = compute_macro_roc(y_val_combined, val_probs_combined, "val")
        test_macro_auc = compute_macro_roc(y_test_combined, test_probs_combined, "test")

        # Confusion matrices (using flattened predictions)
        save_confusion_matrix(y_train_combined, train_preds_combined, "train")
        save_confusion_matrix(y_val_combined, val_preds_combined, "val")
        save_confusion_matrix(y_test_combined, test_preds_combined, "test")

        total_time = timer.stop()

        # Save model and metadata
        timestamp = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
        run_dir = os.path.join(outputs_root, f"run_{timestamp}")
        os.makedirs(run_dir, exist_ok=True)
        cb.save_model(os.path.join(run_dir, 'catboost_bohb_model.cbm'))
        cnn_colon.save(os.path.join(run_dir, 'cnn_colon.h5'))
        cnn_lung.save(os.path.join(run_dir, 'cnn_lung.h5'))

        meta = {
            "config": config,
            "budget": budget,
            "train_accuracy": float(train_accuracy),
            "val_accuracy": float(val_accuracy),
            "test_accuracy": float(test_accuracy),
            "train_macro_auc": float(train_macro_auc),
            "val_macro_auc": float(val_macro_auc),
            "test_macro_auc": float(test_macro_auc),
            "time_feature_sec": round(t_feat,2),
            "time_catboost_sec": round(t_cb,2),
            "time_inference_sec": round(t_pred,2),
            "time_total_sec": round(total_time, 2)
        }
        save_json(meta, os.path.join(run_dir, 'metadata.json'))

        return {"loss": -val_accuracy, "info": meta}

# ============================================================
# MAIN
# ============================================================
if __name__ == "__main__":
    print("\nSTEP 1: Oversampling")
    oversample_all_classes_flat(original_dataset_dir, balanced_temp_dir)

    print("\nSTEP 2: Splitting")
    split_balanced_to_train_val_test(balanced_temp_dir, split_dataset_dir)

    # Display counts after splitting
    print("\n--- Split Counts ---")
    for split_name in ['train','val','test']:
        split_path = os.path.join(split_dataset_dir, split_name)
        if os.path.exists(split_path):
            for cls_name in sorted(os.listdir(split_path)):
                cls_dir = os.path.join(split_path, cls_name)
                if os.path.isdir(cls_dir):
                    print(f"{split_name} {cls_name}: {len(os.listdir(cls_dir))} images")
        else:
            print(f"Warning: {split_path} does not exist.")


    worker = CNNCatBoostWorker(run_id="final_run")
    test_config = {
        'learning_rate': 0.001,
        'dropout_rate': 0.3,
        'weight_decay': 1e-5, # Added weight_decay to config
        'depth': 6,
        'iterations': 500,
        'lr_catboost': 0.1
    }
    result = worker.compute(test_config, budget=1) # Pass test_config and budget
    print("\nRESULT:", result)

# ============================================================
# PYRO4 NAMESERVER (NGROK)
# ============================================================
Pyro4.config.NS_HOST = "0.tcp.in.ngrok.io"
Pyro4.config.NS_PORT = 17178
Pyro4.config.COMMTIMEOUT = 5 # Add a timeout to prevent hanging

try:
    Pyro4.locateNS()
    print("✅ Connected to Pyro4 NameServer")
except Exception as e:
    print("❌ Pyro4 NameServer failed:", e)
