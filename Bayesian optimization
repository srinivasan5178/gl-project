import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import Sequence
import time
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Clear GPU Memory Between Runs
from tensorflow.keras import backend as K
K.clear_session()

# Enable mixed precision for faster training and memory optimization
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Dataset directory (replace with the actual path to your directory)
dataset_dir = "/content/Project_Sample"  # Replace with the actual path to your dataset

# Custom Data Generator to handle both datasets (colon and lung)
class FusionDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5):
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.num_classes = num_classes

    def __len__(self):
        return self.steps_per_epoch  # Number of steps per epoch

    def __getitem__(self, index):
        try:
            # Get a batch of colon data and a batch of lung data
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            # Adjust the labels for lung data to make sure they're within the correct range
            lung_labels = lung_labels + 2  # colon: 0, 1 and lung: 2, 3, 4

            # Concatenate the colon and lung images and labels
            images = np.concatenate([colon_images, lung_images], axis=0)
            labels = np.concatenate([colon_labels, lung_labels], axis=0)

            # Return images and labels as a tuple
            return (images, labels)

        except StopIteration:
            # Reset iterators if they reach the end of data
            self._reset_iterators()
            return self.__getitem__(index)  # Recursively call __getitem__ to get the next batch

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)

# Data generator functions for Colon and Lung datasets
def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )
    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.0,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest')

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )
    return lung_train_gen


# Build a custom CNN model from scratch
def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.0003176, num_classes=5):
    model = models.Sequential()

    # First Convolutional Block
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(layers.MaxPooling2D((2, 2)))

    # Second Convolutional Block
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Third Convolutional Block
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Fourth Convolutional Block (adding depth)
    model.add(layers.Conv2D(256, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Flatten the 3D outputs to 1D
    model.add(layers.Flatten())

    # Fully Connected Layers
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization

    # Output Layer
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Set up data generators
batch_size = 32
steps_per_epoch = 5
num_classes = 5

# Create data generators for colon and lung datasets
colon_train_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_train_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)

# Combine the two generators into one using FusionDataGenerator
train_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Create data generators for validation and testing (same setup as training)
colon_val_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_val_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
val_gen = FusionDataGenerator(colon_val_gen, lung_val_gen, batch_size, steps_per_epoch, num_classes=num_classes)

colon_test_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_test_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
test_gen = FusionDataGenerator(colon_test_gen, lung_test_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Build the custom CNN model
cnn_model = build_cnn_model(input_shape=(64, 64, 3), num_classes=num_classes)

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)

# Track training start time
train_start_time = time.time()

# Train the model
history = cnn_model.fit(
    x=train_gen,
    epochs=500,
    validation_data=val_gen,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Track training end time
train_end_time = time.time()
train_time = train_end_time - train_start_time
print(f"Training Time: {train_time:.2f} seconds")

# Extract features from the CNN model
def extract_features_from_cnn(model, generator, steps):
    features = []
    labels = []
    generator_iterator = iter(generator)
    for _ in range(steps):
        batch_images, batch_labels = next(generator_iterator)
        feature_batch = model.predict(batch_images)
        batch_labels = batch_labels[0] if isinstance(batch_labels, tuple) else batch_labels
        features.append(feature_batch)
        labels.append(batch_labels)
    return np.concatenate(features), np.concatenate(labels)

# Extract features for training, validation, and test datasets
train_features, train_labels = extract_features_from_cnn(cnn_model, train_gen, steps=steps_per_epoch)
val_features, val_labels = extract_features_from_cnn(cnn_model, val_gen, steps=steps_per_epoch)
test_features, test_labels = extract_features_from_cnn(cnn_model, test_gen, steps=steps_per_epoch)

# Define the hyperparameter search space for CatBoost using Bayesian Optimization
search_space = {
    'iterations': Integer(100, 2000),  # Number of boosting iterations
    'learning_rate': Real(0.01, 0.2),  # Learning rate (tune as needed)
    'depth': Integer(3, 10),  # Depth of the trees (tune as needed)
    'l2_leaf_reg': Real(1e-5, 10),  # Regularization parameter for the leaves
}

# Set up Bayesian optimization using BayesSearchCV
bayes_search = BayesSearchCV(
    estimator=CatBoostClassifier(loss_function='MultiClass', verbose=0), # Remove custom_metric here
    search_spaces=search_space,
    n_iter=30,
    cv=3,  # 3
    verbose=2,
)


# Fit the Bayesian optimization model on the training features
bayes_search.fit(train_features, train_labels)

# Get the best hyperparameters found by the optimization process
best_params = bayes_search.best_params_
print(f"Best Hyperparameters: {best_params}")

# Train the CatBoost model using the best found hyperparameters
best_model = bayes_search.best_estimator_

# Train the CatBoost model on the whole training set
#best_model.fit(train_features, train_labels, eval_set=(val_features, val_labels), plot=True)
#bayes_search.fit(train_features, train_labels, fit_params={'custom_metric': ['Accuracy']})

#best_model.fit(train_features, train_labels, eval_set=(val_features, val_labels), plot=True, custom_metric=['Accuracy'])
#best_model.fit(train_features, train_labels, eval_set=(val_features, val_labels), plot=True, eval_metric='Accuracy') # Changed custom_metric to eval_metric

# Train the CatBoost model on the whole training set using the best hyperparameters
#from catboost import CatBoostClassifier

# Initialize the model with eval_metric and a multi-class loss function
best_model = CatBoostClassifier(
    iterations=1000,
    depth=10,
    learning_rate=0.05,
    loss_function='MultiClass',  # Use 'MultiClass' for multi-class classification
    eval_metric='Accuracy'
)

# Fit the model with eval_set (validation data)
best_model.fit(
    train_features,
    train_labels,
    eval_set=(val_features, val_labels),
    plot=True,
    use_best_model=True,
    verbose=100
)


# Save the trained models
cnn_model.save('custom_cnn_model.keras')  # Save the CNN model
best_model.save_model('custom_cnn_cat_model')  # Save the trained CatBoost model

# Make predictions with the optimized CatBoost model
predictions = best_model.predict(test_features)

# Check predictions
print("Predicted Values:", np.unique(predictions))

# Calculate accuracy and print confusion matrix
accuracy = accuracy_score(test_labels, predictions)
print(f"Test Accuracy: {accuracy:.4f}")

# Classification report
print("Test Classification Report:")
print(classification_report(test_labels, predictions))

# Confusion Matrix
cm = confusion_matrix(test_labels, predictions)
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
plt.xticks(np.arange(num_classes), ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'], rotation=45)
plt.yticks(np.arange(num_classes), ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'])
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.tight_layout()
plt.show()

# Check class distribution to ensure balance
print("Train Labels Distribution:", np.unique(train_labels, return_counts=True))
print("Test Labels Distribution:", np.unique(test_labels, return_counts=True))
