# Traning Final code - EfficientNetV2M & DenseNet201 - Srini - 020425
# TPR,FPR,AROUC  - Code
import Pyro4
import ConfigSpace as CS
import numpy as np
from hpbandster.core.worker import Worker
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence
from tensorflow.keras.applications import DenseNet201, EfficientNetV2L,EfficientNetV2M,EfficientNetB7
from tensorflow.keras.regularizers import l2
import seaborn as sns
import matplotlib.pyplot as plt
import socket
import time
from tabulate import tabulate
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split

import time
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc
from keras.callbacks import ReduceLROnPlateau
from tabulate import tabulate
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Traing - EfficientNetB7 - Final Code - 2 class -020425
# Name server connection check - Code 
import Pyro4

# Set up Pyro4 configuration with ngrok details
Pyro4.config.NS_HOST = "4.tcp.ngrok.io"
Pyro4.config.NS_PORT = 19515  # The ngrok port for the Pyro4 nameserver

# Check if the name server is reachable
try:
    # Try to get the nameserver object and ping it
    nameserver = Pyro4.locateNS()
    print("Pyro4 Name Server Connection Success!")
    print(f"NS Host: {Pyro4.config.NS_HOST}")
    print(f"NS Port: {Pyro4.config.NS_PORT}")
except Pyro4.errors.NamingError as e:
    print(f"Connection to Pyro4 Name Server failed: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

# Configure Pyro4 to use the remote nameserver (ngrok)
#Pyro4.config.NS_HOST = "4.tcp.ngrok.io"
#Pyro4.config.NS_PORT = 13459  # The ngrok port for the Pyro4 nameserver

#print(f"Pyro4 NS_HOST: {Pyro4.config.NS_HOST}")
#print(f"Pyro4 NS_PORT: {Pyro4.config.NS_PORT}")


# Custom Data Generator to handle both datasets (colon and lung)
class FusionDataGenerator(Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5, **kwargs):
        super().__init__(**kwargs)  # Add this line
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.num_classes = num_classes
        self.current_index = 0

    def __len__(self):
        return self.steps_per_epoch

    def __next__(self):
        data = self.__getitem__(self.current_index)
        self.current_index += 1
        return data

    def __iter__(self):
        self.current_index = 0
        return self

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.current_index = 0

    def __getitem__(self, index):
        try:
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            lung_labels = lung_labels + 2  # Adjust for correct range
            images = np.concatenate([colon_images, lung_images], axis=0)
            labels = np.concatenate([colon_labels, lung_labels], axis=0)

            return (images, labels)

        except StopIteration:
            self._reset_iterators()
            return self.__getitem__(index)


# Data generator functions for Colon and Lung datasets
def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest',
                                       brightness_range=[0.8, 1.2])  # Added brightness variation

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # EfficientNetV2L requires larger image size
        batch_size=batch_size,
        class_mode='sparse',
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )
    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.0,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest',
                                      brightness_range=[0.8, 1.2])  # Added brightness variation

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # DenseNet201 requires larger image size
        batch_size=batch_size,
        class_mode='sparse',
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )
    return lung_train_gen


# Build a custom CNN model from scratch using DenseNet201 for lung and EfficientNetV2L for colon
def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.0003176, num_classes=5, dropout_rate=0.5, weight_decay=1e-4, dataset_type='lung'):
    # Define a dictionary to map the dataset type to the base model
    model_dict = {
        'lung': DenseNet201,
        'colon': EfficientNetV2M
    }

    # Choose the model based on the dataset type (avoiding elif)
    base_model = model_dict.get(dataset_type)(weights='imagenet', include_top=False, input_shape=input_shape)

    # Freeze base model layers (optional)
    base_model.trainable = False

    # Create the model
    model = models.Sequential()
    model.add(base_model)
    model.add(layers.GlobalAveragePooling2D())  # Global pooling instead of flattening
    model.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(weight_decay)))  # L2 regularization
    model.add(layers.Dropout(dropout_rate))  # Dropout for regularization
    model.add(layers.Dense(256, activation='relu', kernel_regularizer=l2(weight_decay)))
    model.add(layers.Dropout(dropout_rate))
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Define hyperparameter space for optimization
def define_hyperparameter_space():
    config_space = CS.ConfigurationSpace()

    # Hyperparameters for CNN model
    learning_rate = CS.CategoricalHyperparameter('learning_rate', [0.0001, 0.001, 0.01, 0.1])
    dropout_rate = CS.UniformFloatHyperparameter('dropout_rate', 0.0, 0.5)
    weight_decay = CS.UniformFloatHyperparameter('weight_decay', 1e-6, 1e-3)

    # Hyperparameters for CatBoost model
    depth = CS.UniformIntegerHyperparameter('depth', lower=10, upper=500)
    iterations = CS.UniformIntegerHyperparameter('iterations', lower=100, upper=5000)
    lr_catboost = CS.UniformFloatHyperparameter('lr_catboost', lower=0.01, upper=0.2)

    config_space.add([learning_rate, dropout_rate, weight_decay, depth, iterations, lr_catboost])  # Error fix
    return config_space


def extract_features_from_cnn(model, generator, steps):
    features = []
    labels = []
    for _ in range(steps):
        batch_x, batch_y = next(generator)
        batch_features = model.predict(batch_x)
        features.append(batch_features)
        labels.append(batch_y)
    return np.concatenate(features), np.concatenate(labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_names, model_catboost, train_features, train_labels, test_features, test_labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


# Function to calculate the performance metrics (TPR, FPR, Accuracy, etc.) during training
def calculate_training_metrics(model, train_features, train_labels, batch_size=8):
    # Get the model's predictions for the training dataset
    train_predictions = model.predict(train_features)
    train_pred_prob = train_predictions  # For multi-class, you can use model.predict_proba if applicable

    # Convert predictions to binary (if needed for multi-class problems)
    train_pred_bin = np.argmax(train_pred_prob, axis=1)  # Ensure train_labels matches this format (class indices)

    # Ensure train_labels is in the correct format (class indices, not one-hot encoded)
    if len(train_labels.shape) > 1 and train_labels.shape[1] > 1:  # If one-hot encoded
        train_labels = np.argmax(train_labels, axis=1)  # Convert to class indices

    # Calculate confusion matrix
    cm = confusion_matrix(train_labels, train_pred_bin)  # Get full confusion matrix
    
    # Calculate metrics for each class (or aggregate as needed)
    tpr = np.diag(cm) / np.sum(cm, axis=1)  # Class-wise TPR (Recall)
    fpr = (np.sum(cm, axis=0) - np.diag(cm)) / (np.sum(cm) - np.sum(cm, axis=0)) # Class-wise FPR
    accuracy = np.sum(np.diag(cm)) / np.sum(cm)  # Overall accuracy
    precision = precision_score(train_labels, train_pred_bin, average='weighted')
    recall = recall_score(train_labels, train_pred_bin, average='weighted')
    f1 = f1_score(train_labels, train_pred_bin, average='weighted')

    # ROC Curve and AUC (adapt for multi-class if needed)
    # For multi-class, you need to handle ROC AUC differently.
    # Here's an example using 'macro' averaging:
    from sklearn.preprocessing import label_binarize
    train_labels_bin = label_binarize(train_labels, classes=np.unique(train_labels))
    n_classes = train_labels_bin.shape[1]
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(train_labels_bin[:, i], train_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(train_labels_bin.ravel(), train_pred_prob.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

     # Compute macro-average ROC curve and ROC area (average across classes)
    # First aggregate all false positive rates
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

    # Then interpolate all ROC curves at this points
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

    # Finally average it and compute AUC
    mean_tpr /= n_classes

    fpr["macro"] = all_fpr
    tpr["macro"] = mean_tpr
    roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

    # Use macro-averaged values for TPR, FPR, and ROC AUC
    tpr_macro = tpr["macro"]
    fpr_macro = fpr["macro"]
    roc_auc_macro = roc_auc["macro"]

    return tpr_macro, fpr_macro, accuracy, precision, recall, f1, roc_auc_macro # Return macro-averaged values


# Function to extract features from the CNN model
def extract_features_from_cnn(model, data_generator, steps):
    features = []
    labels = []
    
    # Collect features and labels for the training dataset
    for step in range(steps):
        data, label = data_generator.__getitem__(step)  # Get the batch data and labels
        features.append(data)
        labels.append(label)

    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    
    return features, labels


# Define the CNN model (example)
def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.001, num_classes=5, dropout_rate=0.5, weight_decay=0.0001):
    from keras.models import Sequential
    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
    from keras.optimizers import Adam

    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model


# Function to load and split data into train, validation, and test sets
def load_and_split_data():
    # Assuming you're using numpy arrays for the dataset
    # Replace with your actual image loading code and labels
    data = np.random.rand(1000, 64, 64, 3)  # Example random data, replace with actual images
    labels = np.random.randint(0, 5, size=(1000,))  # Example labels, replace with actual labels

    # Split data into 70% train, 15% validation, 15% test
    X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42, stratify=labels)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

    # One-hot encode the labels using tf.keras.utils.to_categorical
    from tensorflow.keras.utils import to_categorical

    y_train = to_categorical(y_train, num_classes=5)
    y_val = to_categorical(y_val, num_classes=5)
    y_test = to_categorical(y_test, num_classes=5)

    return X_train, X_val, X_test, y_train, y_val, y_test


# Worker for training the model and calculating metrics
class CNNCatBoostWorker:
    def __init__(self, run_id=None):  # Add __init__ method
        self.run_id = run_id

    def compute(self, config, budget, *args, **kwargs):
        # Load and split the data
        X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data()

        # Define hyperparameters from config
        learning_rate = config['learning_rate']
        dropout_rate = config['dropout_rate']
        weight_decay = config['weight_decay']
        depth = config['depth']
        iterations = config['iterations']
        lr_catboost = config['lr_catboost']

        # Build and compile the CNN model
        cnn_model = build_cnn_model(input_shape=(64, 64, 3), learning_rate=learning_rate, num_classes=5, dropout_rate=dropout_rate, weight_decay=weight_decay)

        # Learning rate scheduler
        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

        # Train the model with training and validation data
        start_train_time = time.time()
        history = cnn_model.fit(X_train, y_train, epochs=70, validation_data=(X_val, y_val), batch_size=32, callbacks=[lr_scheduler], verbose=0)
        end_train_time = time.time()
        
        train_time = end_train_time - start_train_time

        # Calculate training features and labels to evaluate metrics
        tpr, fpr, accuracy, precision, recall, f1, roc_auc = calculate_training_metrics(cnn_model, X_train, y_train)

        # Get training loss (last loss value)
        train_loss = history.history['loss'][-1]

        # Evaluate the model on the test data
        test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)

        # Prepare the result dictionary with all metrics
        result = {
            'train_accuracy': accuracy,
            'train_precision': precision,
            'train_recall': recall,
            'train_f1': f1,
            'train_roc_auc': roc_auc,
            'train_tpr': tpr,
            'train_fpr': fpr,
            'train_loss': train_loss,
            'train_time': train_time,
            'test_accuracy': test_accuracy,
            'test_loss': test_loss
        }

        # Print the metrics table
        print_metrics_table(result)

        return result



# Function to print the training metrics in a well-formatted table
def print_metrics_table(result):
    headers = ["Metric", "Training", "Test"]

    # Check if train_tpr and train_fpr are dictionaries
    # If so, convert to a single value (e.g., macro-average) for printing
    train_tpr = result['train_tpr']
    train_fpr = result['train_fpr']

    # Modify to check if train_tpr is a dictionary or array
    # and then convert to scalar or loop through it
    if isinstance(train_tpr, dict):
        train_tpr = np.mean(list(train_tpr.values()))  # Or another aggregation method
    elif isinstance(train_tpr, np.ndarray):
        train_tpr = train_tpr.mean()  # Take the mean if it's an array

    # Modify to check if train_fpr is a dictionary or array
    # and then convert to scalar or loop through it
    if isinstance(train_fpr, dict):
        train_fpr = np.mean(list(train_fpr.values()))  # Or another aggregation method
    elif isinstance(train_fpr, np.ndarray):
        train_fpr = train_fpr.mean()  # Take the mean if it's an array


    rows = [
        ["Accuracy", f"{result['train_accuracy']:.4f}", f"{result['test_accuracy']:.4f}"],
        ["Precision", f"{result['train_precision']:.4f}", "-"],
        ["Recall", f"{result['train_recall']:.4f}", "-"],
        ["F1 Score", f"{result['train_f1']:.4f}", "-"],
        ["ROC AUC", f"{result['train_roc_auc']:.4f}", "-"],
        ["TPR (True Positive Rate)", f"{train_tpr:.4f}", "-"],  # Use the converted value
        ["FPR (False Positive Rate)", f"{train_fpr:.4f}", "-"],  # Use the converted value
        ["Loss", f"{result['train_loss']:.4f}", f"{result['test_loss']:.4f}"],
        ["Time (s)", f"{result['train_time']:.2f}", "-"]
    ]

    print(tabulate(rows, headers=headers, tablefmt="grid"))


# Example usage:
if __name__ == "__main__":
    # Instantiate the worker and define the hyperparameter space
    worker = CNNCatBoostWorker(run_id="worker_1")
    config_space = define_hyperparameter_space()

    # Test the worker's computation method
    config = {'learning_rate': 0.001, 'dropout_rate': 0.3, 'weight_decay': 1e-5, 'depth': 10, 'iterations': 1000, 'lr_catboost': 0.1}
    result = worker.compute(config=config, budget=25)
    print(result
