# Final Code the Test - 020425 DenseNet201 & EfficientNetV2L 
import Pyro4
import ConfigSpace as CS
import numpy as np
from hpbandster.core.worker import Worker
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay, roc_curve
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence
from tensorflow.keras.applications import DenseNet201, EfficientNetB7, EfficientNetV2L
from tensorflow.keras.regularizers import l2
import seaborn as sns
import matplotlib.pyplot as plt
import socket
import time
from tabulate import tabulate
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split


# Configure Pyro4 to use the remote nameserver (ngrok)
Pyro4.config.NS_HOST = "4.tcp.ngrok.io"
Pyro4.config.NS_PORT = 19515  # The ngrok port for the Pyro4 nameserver

print(f"Pyro4 NS_HOST: {Pyro4.config.NS_HOST}")
print(f"Pyro4 NS_PORT: {Pyro4.config.NS_PORT}")


# Custom Data Generator to handle both datasets (colon and lung)
class FusionDataGenerator(Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5, **kwargs):
        super().__init__(**kwargs)
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.num_classes = num_classes
        self.current_index = 0

    def __len__(self):
        return self.steps_per_epoch

    def __next__(self):
        data = self.__getitem__(self.current_index)
        self.current_index += 1
        return data

    def __iter__(self):
        self.current_index = 0
        return self

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.current_index = 0

    def __getitem__(self, index):
        try:
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            lung_labels = lung_labels + 2  # Adjust for correct range
            images = np.concatenate([colon_images, lung_images], axis=0)
            labels = np.concatenate([colon_labels, lung_labels], axis=0)

            return (images, labels)

        except StopIteration:
            self._reset_iterators()
            return self.__getitem__(index)


# Data generator functions for Colon and Lung datasets
def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest',
                                       brightness_range=[0.8, 1.2])  # Added brightness variation

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # EfficientNetV2L requires larger image size
        batch_size=batch_size,
        class_mode='sparse',
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )
    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.0,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest',
                                      brightness_range=[0.8, 1.2])  # Added brightness variation

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # DenseNet201 requires larger image size
        batch_size=batch_size,
        class_mode='sparse',
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )
    return lung_train_gen


# Build a custom CNN model from scratch using DenseNet201 for lung and EfficientNetV2L for colon
def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.0003176, num_classes=5, dropout_rate=0.5, weight_decay=1e-4, dataset_type='lung'):
    # Define a dictionary to map the dataset type to the base model
    model_dict = {
        'lung': DenseNet201,
        'colon': EfficientNetV2L
    }

    # Choose the model based on the dataset type (avoiding elif)
    base_model = model_dict.get(dataset_type)(weights='imagenet', include_top=False, input_shape=input_shape)

    # Freeze base model layers (optional)
    base_model.trainable = False

    # Create the model
    model = models.Sequential()
    model.add(base_model)
    model.add(layers.GlobalAveragePooling2D())  # Global pooling instead of flattening
    model.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(weight_decay)))  # L2 regularization
    model.add(layers.Dropout(dropout_rate))  # Dropout for regularization
    model.add(layers.Dense(256, activation='relu', kernel_regularizer=l2(weight_decay)))
    model.add(layers.Dropout(dropout_rate))
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Define hyperparameter space for optimization
def define_hyperparameter_space():
    config_space = CS.ConfigurationSpace()

    # Hyperparameters for CNN model
    learning_rate = CS.CategoricalHyperparameter('learning_rate', [0.0001, 0.001, 0.01, 0.1])
    dropout_rate = CS.UniformFloatHyperparameter('dropout_rate', 0.0, 0.5)
    weight_decay = CS.UniformFloatHyperparameter('weight_decay', 1e-6, 1e-3)

    # Hyperparameters for CatBoost model
    depth = CS.UniformIntegerHyperparameter('depth', lower=10, upper=500)
    iterations = CS.UniformIntegerHyperparameter('iterations', lower=100, upper=5000)
    lr_catboost = CS.UniformFloatHyperparameter('lr_catboost', lower=0.01, upper=0.2)

    config_space.add([learning_rate, dropout_rate, weight_decay, depth, iterations, lr_catboost])  # Error fix
    return config_space


def extract_features_from_cnn(model, generator, steps):
    features = []
    labels = []
    for _ in range(steps):
        batch_x, batch_y = next(generator)
        batch_features = model.predict(batch_x)
        features.append(batch_features)
        labels.append(batch_y)
    return np.concatenate(features), np.concatenate(labels)


# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_names, model_catboost, train_features, train_labels, test_features, test_labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


# Objective function for BOHB optimization
from sklearn.metrics import confusion_matrix
from tabulate import tabulate

class CNNCatBoostWorker(Worker):
    def compute(self, config, budget, *args, **kwargs):
        # Ensure correct dataset paths are used
        colon_dataset_dir = '/content/Project_Sample_classified/train/colon'
        lung_dataset_dir = '/content/Project_Sample_classified/train/lung'
        colon_dataset_dir_val = '/content/Project_Sample_classified/val/colon'
        lung_dataset_dir_val = '/content/Project_Sample_classified/val/lung'
        colon_dataset_dir_test = '/content/Project_Sample_classified/test/colon'  # Test dataset path
        lung_dataset_dir_test = '/content/Project_Sample_classified/test/lung'      # Test dataset path
        
        # Create validation data generators
        colon_train_gen_val = create_data_generator_colon(colon_dataset_dir_val, batch_size=8)
        lung_train_gen_val = create_data_generator_lung(lung_dataset_dir_val, batch_size=8)
        val_gen = FusionDataGenerator(colon_train_gen_val, lung_train_gen_val, batch_size=8, steps_per_epoch=5, num_classes=5)

        # Create test data generators
        colon_train_gen_test = create_data_generator_colon(colon_dataset_dir_test, batch_size=8)
        lung_train_gen_test = create_data_generator_lung(lung_dataset_dir_test, batch_size=8)
        test_gen = FusionDataGenerator(colon_train_gen_test, lung_train_gen_test, batch_size=8, steps_per_epoch=5, num_classes=5)

        # Extract hyperparameters
        learning_rate = config['learning_rate']
        dropout_rate = config['dropout_rate']
        weight_decay = config['weight_decay']
        depth = config['depth']
        iterations = config['iterations']
        lr_catboost = config['lr_catboost']

        # Create data generators for training
        colon_train_gen = create_data_generator_colon(colon_dataset_dir, batch_size=8)
        lung_train_gen = create_data_generator_lung(lung_dataset_dir, batch_size=8)
        train_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size=8, steps_per_epoch=5, num_classes=5)

        # Build and compile the CNN model
        cnn_model = build_cnn_model(input_shape=(64, 64, 3), learning_rate=learning_rate, num_classes=5, dropout_rate=dropout_rate, weight_decay=weight_decay)

        # Extract features from CNN for training CatBoost
        train_features, train_labels = extract_features_from_cnn(cnn_model, train_gen, steps=5)

        # Implement learning rate scheduling
        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

        # **Training the model**
        start_val_time = time.time()
        history = cnn_model.fit(train_gen, epochs=70, validation_data=val_gen, steps_per_epoch=5, validation_steps=5, callbacks=[lr_scheduler], verbose=0)
        end_val_time = time.time()
        val_time = end_val_time - start_val_time

        # Model evaluation on validation data
        val_loss, val_acc = cnn_model.evaluate(val_gen)

        # Initialize CatBoost model
        model_catboost = CatBoostClassifier(iterations=iterations, learning_rate=lr_catboost, depth=depth)

        # Extract features from CNN model for CatBoost
        cnn_model.trainable = False
        train_features_cnn, train_labels_cnn = extract_features_from_cnn(cnn_model, train_gen, steps=5)

        # Train CatBoost model
        model_catboost.fit(train_features_cnn, train_labels_cnn)

        # **Validation Results**
        val_features_cnn, val_labels_cnn = extract_features_from_cnn(cnn_model, val_gen, steps=5)
        val_predictions = model_catboost.predict(val_features_cnn)

        # Calculate confusion matrix for validation
        cm_val = confusion_matrix(val_labels_cnn, val_predictions)
        
        # Calculate metrics for validation
        val_accuracy = accuracy_score(val_labels_cnn, val_predictions)
        val_precision = precision_score(val_labels_cnn, val_predictions, average='weighted')
        val_recall = recall_score(val_labels_cnn, val_predictions, average='weighted')
        val_f1 = f1_score(val_labels_cnn, val_predictions, average='weighted')

        # **Test Results**
        start_test_time = time.time()
        test_features_cnn, test_labels_cnn = extract_features_from_cnn(cnn_model, test_gen, steps=5)
        test_predictions = model_catboost.predict(test_features_cnn)
        end_test_time = time.time()
        test_time = end_test_time - start_test_time

        # Calculate confusion matrix for test
        cm_test = confusion_matrix(test_labels_cnn, test_predictions)

        # Calculate metrics for test
        test_accuracy = accuracy_score(test_labels_cnn, test_predictions)
        test_precision = precision_score(test_labels_cnn, test_predictions, average='weighted')
        test_recall = recall_score(test_labels_cnn, test_predictions, average='weighted')
        test_f1 = f1_score(test_labels_cnn, test_predictions, average='weighted')
        test_loss = cnn_model.evaluate(test_gen)

        # Calculate AUC for test data
        from sklearn.preprocessing import label_binarize

        test_pred_prob = model_catboost.predict_proba(test_features_cnn)
        test_labels_cnn_binarized = label_binarize(test_labels_cnn, classes=np.unique(test_labels_cnn))
        test_auc_score = roc_auc_score(test_labels_cnn_binarized, test_pred_prob, average='weighted', multi_class='ovr')

        # Print results in tabular format
        results = [
            ["Validation Accuracy", val_acc],
            ["Validation F1 Score", val_f1],
            ["Validation Precision", val_precision],
            ["Validation Recall", val_recall],
            ["Validation AUC", test_auc_score],
            ["Validation Time", f"{val_time:.2f} seconds"],
            ["Validation Loss", val_loss],
            ["Test Accuracy", test_accuracy],
            ["Test F1 Score", test_f1],
            ["Test Precision", test_precision],
            ["Test Recall", test_recall],
            ["Test AUC", test_auc_score],
            ["Test Time", f"{test_time:.2f} seconds"],
            ["Test Loss", test_loss]
        ]

        print(tabulate(results, headers=["Metric", "Value"], tablefmt="grid"))

        # Optionally plot confusion matrix for test data
        plot_confusion_matrix(cm_test, class_names=['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'], model_catboost=model_catboost, train_features=train_features_cnn, train_labels=train_labels_cnn, test_features=test_features_cnn, test_labels=test_labels_cnn)

        return {
            "validation_auc": test_auc_score,
            "test_accuracy": test_accuracy,
            "test_f1": test_f1,
            "test_precision": test_precision,
            "test_recall": test_recall,
            "test_time": test_time
        }


         # Print results in tabular format
        print(tabulate(results, headers=["Metric", "Value"], tablefmt="grid"))

        # Optionally plot confusion matrix
        plot_confusion_matrix(cm, class_names=['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'], model_catboost=model_catboost, train_features=test_features_cnn, train_labels=test_labels_cnn, test_features=test_features_cnn, test_labels=test_labels_cnn)
        return {"test_auc": auc_score} # Corrected indentation

    # Function to compute and print metrics in tabular format
    def print_metrics_table(self, result):
        # Define the headers for the table
        headers = ["Metric", "Training", "Validation", "Test"]

        # Define the rows of the table, formatted to 4 decimal places
        rows = [
            ["Accuracy", f"{result['train_accuracy']:.4f}", f"{result['val_accuracy']:.4f}", f"{result['test_accuracy']:.4f}"],
            ["Precision", f"{result['train_precision']:.4f}", f"{result['val_precision']:.4f}", f"{result['test_precision']:.4f}"],
            ["Recall", f"{result['train_recall']:.4f}", f"{result['val_recall']:.4f}", f"{result['test_recall']:.4f}"],
            ["F1", f"{result['train_f1']:.4f}", f"{result['val_f1']:.4f}", f"{result['test_f1']:.4f}"],
            ["Loss", f"{result['train_loss']:.4f}", f"{result['val_loss']:.4f}", f"{result['test_loss']:.4f}"],
            ["Training Time (s)", f"{result['train_time']:.4f}", "", ""],
            ["CatBoost Train Time (s)", f"{result['catboost_train_time']:.4f}", "", ""]
        ]

        # Print the table using the tabulate library
        print(tabulate(rows, headers=headers, tablefmt="fancy_grid"))


# Example usage:
if __name__ == "__main__":
    # Instantiate the worker and define the hyperparameter space
    worker = CNNCatBoostWorker(run_id="worker_1")
    config_space = define_hyperparameter_space()

    # Test the worker's computation method
    config = {'learning_rate': 0.001, 'dropout_rate': 0.3, 'weight_decay': 1e-5, 'depth': 10, 'iterations': 1000, 'lr_catboost': 0.1}
    result = worker.compute(config=config, budget=25)
    print(result)
