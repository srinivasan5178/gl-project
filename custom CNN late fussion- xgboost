import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import Sequence
import time
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Clear GPU Memory Between Runs
from tensorflow.keras import backend as K
K.clear_session()

# Enable mixed precision for faster training and memory optimization
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Dataset directory (replace with the actual path to your directory)
dataset_dir = "/content/Project_Sample"  # Replace with the actual path to your dataset

# Custom Data Generator to handle both datasets (colon and lung)
class FusionDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5):
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.num_classes = num_classes

    def __len__(self):
        return self.steps_per_epoch  # Number of steps per epoch

    def __getitem__(self, index):
        try:
            # Get a batch of colon data and a batch of lung data
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            # Adjust the labels for lung data to make sure they're within the correct range
            lung_labels = lung_labels + 2  # colon: 0, 1 and lung: 2, 3, 4

            # Concatenate the colon and lung images and labels
            images = np.concatenate([colon_images, lung_images], axis=0)
            labels = np.concatenate([colon_labels, lung_labels], axis=0)

            # Return images and labels as a tuple
            return (images, labels)

        except StopIteration:
            # Reset iterators if they reach the end of data
            self._reset_iterators()
            return self.__getitem__(index)  # Recursively call __getitem__ to get the next batch

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)

# Data generator functions for Colon and Lung datasets
def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )
    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.0,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest')

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )
    return lung_train_gen


# Build a custom CNN model from scratch
def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.0003176, num_classes=5):
    model = models.Sequential()

    # First Convolutional Block
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(layers.MaxPooling2D((2, 2)))

    # Second Convolutional Block
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Third Convolutional Block
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Fourth Convolutional Block (adding depth)
    model.add(layers.Conv2D(256, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Flatten the 3D outputs to 1D
    model.add(layers.Flatten())

    # Fully Connected Layers
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization

    # Output Layer
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Set up data generators
batch_size = 32
steps_per_epoch = 5
num_classes = 5

# Create data generators for colon and lung datasets
colon_train_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_train_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)

# Combine the two generators into one using FusionDataGenerator
train_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Create data generators for validation and testing (same setup as training)
colon_val_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_val_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
val_gen = FusionDataGenerator(colon_val_gen, lung_val_gen, batch_size, steps_per_epoch, num_classes=num_classes)

colon_test_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_test_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
test_gen = FusionDataGenerator(colon_test_gen, lung_test_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Build the custom CNN model
cnn_model = build_cnn_model(input_shape=(64, 64, 3), num_classes=num_classes)

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)

# Track training start time
train_start_time = time.time()

# Train the model
history = cnn_model.fit(
    x=train_gen,
    epochs=200,
    validation_data=val_gen,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Track training end time
train_end_time = time.time()
train_time = train_end_time - train_start_time
print(f"Training Time: {train_time:.2f} seconds")

# Extract features from the CNN model
def extract_features_from_cnn(model, generator, steps):
    features = []
    labels = []
    generator_iterator = iter(generator)
    for _ in range(steps):
        batch_images, batch_labels = next(generator_iterator)
        feature_batch = model.predict(batch_images)
        batch_labels = batch_labels[0] if isinstance(batch_labels, tuple) else batch_labels
        features.append(feature_batch)
        labels.append(batch_labels)
    return np.concatenate(features), np.concatenate(labels)

# Extract features for training, validation, and test datasets
train_features, train_labels = extract_features_from_cnn(cnn_model, train_gen, steps=steps_per_epoch)
val_features, val_labels = extract_features_from_cnn(cnn_model, val_gen, steps=steps_per_epoch)
test_features, test_labels = extract_features_from_cnn(cnn_model, test_gen, steps=steps_per_epoch)

# Train CatBoost Classifier
model_catboost = CatBoostClassifier(
    iterations=2000,  # Number of boosting iterations
    learning_rate=0.1,  # Learning rate (tune as needed)
    depth=6,  # Depth of the trees (tune as needed)
    loss_function='MultiClass',  # For multi-class classification
    custom_metric=['Accuracy'],
    verbose=200  # Print progress every 200 iterations
)

# Fit the CatBoost model on the extracted features
model_catboost.fit(train_features, train_labels, eval_set=(val_features, val_labels), plot=True)


# Save the trained models
cnn_model.save('custom_cnn_model.keras')  # Save the CNN model
model_catboost.save_model('custom_cnn_cat_model')  # Save the trained CatBoost model

# You can later load these models like this:
# For CNN Model:
# from tensorflow.keras.models import load_model
# cnn_model = load_model('custom_cnn_model.keras')

# For CatBoost Model:
# model_catboost = CatBoostClassifier()
# model_catboost.load_model('custom_cnn_cat_model')

# Make predictions with the CatBoost model
predictions = model_catboost.predict(test_features)

# Check predictions
print("Predicted Values:", np.unique(predictions))

# Calculate accuracy and print confusion matrix
accuracy = accuracy_score(test_labels, predictions)
print(f"Test Accuracy: {accuracy:.4f}")

# Classification report
print("Test Classification Report:")
print(classification_report(test_labels, predictions))

# Confusion Matrix
cm = confusion_matrix(test_labels, predictions)
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
plt.xticks(np.arange(num_classes), ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'], rotation=45)
plt.yticks(np.arange(num_classes), ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc'])
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.tight_layout()
plt.show()

# Check class distribution to ensure balance
print("Train Labels Distribution:", np.unique(train_labels, return_counts=True))
print("Test Labels Distribution:", np.unique(test_labels, return_counts=True))
