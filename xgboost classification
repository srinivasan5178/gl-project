# DenseNet201&EfficientNetV2L+ BOHB - xgboost classification
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Dropout, Concatenate, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from hpbandster.optimizers import BOHB


# Clear GPU Memory Between Runs
from tensorflow.keras import backend as K
K.clear_session()

# Enable mixed precision to handle large models with lower memory usage
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Dataset directory
dataset_dir = "/content/Project_Sample"  # Replace with the actual path to your directory

# Define the FusionDataGenerator class
class FusionDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5):
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)  # Create iterator for colon_gen
        self.lung_iterator = iter(self.lung_gen)    # Create iterator for lung_gen
        self.num_classes = num_classes  # Store the number of classes

    def __len__(self):
        return self.steps_per_epoch  # Number of steps per epoch

    def __getitem__(self, index):
        try:
            # Get the next batch from both iterators
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            # Ensure consistent batch size for combined images and labels
            assert colon_images.shape[0] == self.batch_size, f"Shape mismatch in colon images: {colon_images.shape[0]}"
            assert lung_images.shape[0] == self.batch_size, f"Shape mismatch in lung images: {lung_images.shape[0]}"

            # Adjust lung labels by adding an offset (to ensure no overlap)
            lung_labels = lung_labels + 2  # Offset lung labels to avoid overlap with colon labels

            # Combine labels using np.where to create a single target vector
            combined_labels = np.where(colon_labels == 0, colon_labels, lung_labels)


            # Instead of concatenating, stack the labels to create a (batch_size, 2) shape
            #combined_labels = np.stack([colon_labels, lung_labels], axis=1)

            # Return the separate inputs (colon_images, lung_images) and the stacked labels
            return (
                {'colon_input': tf.convert_to_tensor(colon_images, dtype=tf.float32),
                 'lung_input': tf.convert_to_tensor(lung_images, dtype=tf.float32)},
                tf.convert_to_tensor(combined_labels, dtype=tf.int32)
            )

        except StopIteration:
            # Reset iterators when reaching the end of an epoch
            self._reset_iterators()
            return self.__getitem__(index)  # Recursively call __getitem__ to get the next batch

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)


# Data generator functions
def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # Resize to 64x64
        batch_size=batch_size,
        class_mode='sparse',  # Changed to sparse
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )

    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.2,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest')

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),  # Resize to 64x64
        batch_size=batch_size,
        class_mode='sparse',  # Changed to sparse
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )

    return lung_train_gen


# Base model creation functions with fine-tuning
def create_base_model1(input_shape=(64, 64, 3), output_dim=128):
    base_model_colon = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=input_shape)
    for layer in base_model_colon.layers[:400]:
        layer.trainable = False

    x = base_model_colon.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    x = Dense(output_dim, activation='relu')(x)  # Add a Dense layer to match output size
    return Model(inputs=base_model_colon.input, outputs=x)

def create_base_model2(input_shape=(64, 64, 3), output_dim=128):
    base_model_lung = tf.keras.applications.EfficientNetV2L(weights='imagenet', include_top=False, input_shape=input_shape)
    for layer in base_model_lung.layers[:400]:
        layer.trainable = False

    y = base_model_lung.output
    y = GlobalAveragePooling2D()(y)
    y = Dropout(0.5)(y)
    y = Dense(output_dim, activation='relu')(y)  # Add a Dense layer to match output size
    return Model(inputs=base_model_lung.input, outputs=y)


# Build the model with predefined best hyperparameters
def build_model(dense1_units=512, dropout_rate=0.2, learning_rate=0.0003176, num_classes=5):
    img_size_colon = (64, 64, 3)
    img_size_lung = (64, 64, 3)

    colon_input = Input(shape=img_size_colon, name='colon_input')
    lung_input = Input(shape=img_size_lung, name='lung_input')

    base_model_colon = create_base_model1(img_size_colon, output_dim=128)
    base_model_lung = create_base_model2(img_size_lung, output_dim=128)

    colon_features = base_model_colon(colon_input)
    lung_features = base_model_lung(lung_input)

    # Concatenate features from both models
    concatenated_features = Concatenate(axis=1)([colon_features, lung_features])
    normalized_features = BatchNormalization()(concatenated_features)

    # Dense layers with predefined hyperparameters
    dense1 = Dense(dense1_units, activation='relu', kernel_regularizer=regularizers.l2(0.001))(normalized_features)
    dense1 = Dropout(dropout_rate)(dense1)
    dense1 = BatchNormalization()(dense1)

    dense2 = Dense(256, activation='relu')(dense1)
    dense2 = BatchNormalization()(dense2)

    dense3 = Dense(128, activation='relu')(dense2)

    # Final output layer
    output = Dense(num_classes, activation='softmax')(dense3)

    model = Model(inputs=[colon_input, lung_input], outputs=output)

    # Compile the model with predefined learning rate
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Instantiate data generators for training and validation
batch_size = 8
steps_per_epoch = 5
num_classes = 5  # Changed num_classes to 4

# Call the data generator functions and assign their output to the respective variables
colon_train_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_train_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)

# Create FusionDataGenerator for training
train_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Create data generators for validation
colon_val_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_val_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
val_gen = FusionDataGenerator(colon_val_gen, lung_val_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Create data generators for testing
colon_test_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_test_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)
test_gen = FusionDataGenerator(colon_test_gen, lung_test_gen, batch_size, steps_per_epoch, num_classes=num_classes)

# Build the model with predefined best values
best_model = build_model(dense1_units=512, dropout_rate=0.2, learning_rate=0.0003176, num_classes=num_classes)

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

history = best_model.fit(
   x=train_gen,
   epochs=25,  # Number of epochs
   validation_data=val_gen,
   callbacks=[early_stopping, reduce_lr],
   verbose=1
)

# Save the trained model
best_model.save('best_model.keras')  # Save the entire model

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import numpy as np
from tensorflow.keras.models import Model

def extract_features(generator, model, num_samples):
    features = []
    labels = []

    # Create an iterator from the generator
    generator_iterator = iter(generator)

    # Create a feature extractor model from the passed in model
    feature_extractor = Model(inputs=model.input, outputs=model.layers[-4].output)

    # Using the generator iterator, iterate through the number of samples
    for _ in range(num_samples):
        try:
            # Get the next batch
            batch = next(generator_iterator)

            # Check if it's from FusionDataGenerator or ImageDataGenerator
            if isinstance(batch, tuple) and len(batch) == 2 and isinstance(batch[0], tuple) and isinstance(batch[1], tuple):
                (colon_images, lung_images), (colon_labels, lung_labels) = batch  # Handle FusionDataGenerator output

                # Predict using both image sets, only extract features from base model
                feature = feature_extractor.predict([colon_images, lung_images])

                # Check feature shape before reshaping
                print(f"Feature shape before reshaping: {feature.shape}")

                # Reshape the features to 2D before appending
                feature = feature.reshape(feature.shape[0], -1)  # Flatten the last two dimensions
                features.append(feature)

                # Combine labels, keeping colon labels [0,1] and lung labels as [2,3]
                combined_labels = np.concatenate([colon_labels, lung_labels + 2])
                labels.append(combined_labels)

            elif isinstance(batch, tuple) and len(batch) == 2:  # Handle ImageDataGenerator output
                images, label = batch

                # Predict using the images, only extract features from base model
                feature = feature_extractor.predict([images, np.zeros_like(images)])  # dummy lung input

                # Check feature shape before reshaping
                print(f"Feature shape before reshaping: {feature.shape}")

                # Reshape the features to 2D before appending
                feature = feature.reshape(feature.shape[0], -1)  # Flatten the last two dimensions
                features.append(feature)
                labels.append(label)  # Append label

            else:
                raise ValueError("Unexpected batch format from generator.")
        except StopIteration:
            # Handle end of generator
            break

    # Concatenate features and labels into numpy arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)

    return features, labels

# Extract features and labels from the colon and lung datasets
#colon_train_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon_image_sets"), batch_size)
#lung_train_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung_image_sets"), batch_size)


colon_train_gen = create_data_generator_colon(os.path.join(dataset_dir, "colon"), batch_size)
lung_train_gen = create_data_generator_lung(os.path.join(dataset_dir, "lung"), batch_size)

# Specify number of samples to extract (ensure consistency)
num_samples = 1000  # Adjust this number to the number of samples you want to extract

#colon_features, colon_labels = extract_features(colon_train_gen, best_model, num_samples=num_samples)
#lung_features, lung_labels = extract_features(lung_train_gen, best_model, num_samples=num_samples)

colon_features, colon_labels = extract_features(colon_train_gen, best_model, num_samples=num_samples)
lung_features, lung_labels = extract_features(lung_train_gen, best_model, num_samples=num_samples)

# Check the shape of the features and labels for consistency
print(f"Colon Features Shape: {colon_features.shape}")
print(f"Colon Labels Shape: {colon_labels.shape}")
print(f"Lung Features Shape: {lung_features.shape}")
print(f"Lung Labels Shape: {lung_labels.shape}")

# Ensure that both feature arrays and label arrays have the same number of samples
assert colon_features.shape[0] == colon_labels.shape[0], f"Mismatch in colon features and labels: {colon_features.shape[0]} vs {colon_labels.shape[0]}"
assert lung_features.shape[0] == lung_labels.shape[0], f"Mismatch in lung features and labels: {lung_features.shape[0]} vs {lung_labels.shape[0]}"

# Combine the features and labels from both datasets
features = np.concatenate([colon_features, lung_features], axis=0)
labels = np.concatenate([colon_labels, lung_labels], axis=0)

# Check the shape of the combined features and labels
print(f"Combined Features Shape: {features.shape}")
print(f"Combined Labels Shape: {labels.shape}")

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)


from sklearn.utils.class_weight import compute_class_weight

# Reshape y_train to be a 1D array
y_train_reshaped = y_train.reshape(-1)

# Compute class weights using the reshaped array
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train_reshaped),
    y=y_train_reshaped
)

# Train an XGBoost model with Hyperparameter Tuning
xgb_model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=5,
    eval_metric='mlogloss',
    max_depth=8,  # Chosen depth for the decision tree
    learning_rate=0.1,  # Learning rate for boosting
    n_estimators=500,  # Number of trees to train
    subsample=0.7,  # Fraction of samples for training each tree
    colsample_bytree=0.7,  # Fraction of features used for each tree
    gamma=0.1  # Regularization parameter for trees
)

# Check the shape of the split data
print(f"X_train Shape: {X_train.shape}")
print(f"X_val Shape: {X_val.shape}")
print(f"y_train Shape: {y_train.shape}")
print(f"y_val Shape: {y_val.shape}")

# Train an XGBoost model on the features
#xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes) # Changed num_classes to 4
xgb_model.fit(X_train, y_train)

# Evaluate the XGBoost model
y_pred_xgb = xgb_model.predict(X_val)
xgb_val_accuracy = xgb_model.score(X_val, y_val)
print(f"XGBoost Validation Accuracy: {xgb_val_accuracy:.4f}")


# Define the class names to class indices mapping
class_mapping = {
    'colon_aca': 1,
    'colon_n': 1,
    'lung_aca': 1,
    'lung_n': 1,
    'lung_scc': 1
}

# Get unique classes in y_val and y_pred_xgb
unique_classes = np.unique(np.concatenate((y_val, y_pred_xgb)))

# Get all the class names from class_mapping
all_class_names = list(class_mapping.keys())

# Filter class_mapping to include only the present classes in unique_classes
present_class_mapping = [class_name for class_name in all_class_names if class_mapping[class_name] in unique_classes]

# Ensure target_names match the present classes
target_names = present_class_mapping if present_class_mapping else all_class_names

# Print the final target names for verification
#print(f"Final Target Names: {target_names}")


# Ensure target_names always include all 5 classes
target_names = list(class_mapping.keys())  # This includes all the class names from class_mapping

# Print the final target names for verification
print(f"Final Target Names: {target_names}")

# Print classification report
print(f"Classification Report:")
print(classification_report(y_val, y_pred_xgb,
                            target_names=target_names,
                            labels=list(class_mapping.values()),  # Ensure all class indices are used
                            zero_division=0))  # Handle zero division if needed

# Create a confusion matrix for validation
cm_val = confusion_matrix(y_val, y_pred_xgb, labels=list(class_mapping.values()))  # Use all class indices
ConfusionMatrixDisplay(confusion_matrix=cm_val,
                       display_labels=target_names).plot()  # Use all class names
plt.title("Validation Confusion Matrix for XGBoost Model")
plt.show()


## HBOH configuration ##

import hpbandster as hp
from hpbandster.optimizers import BOHB
from hpbandster.core import HyperParameterOptimizationJob
from hpbandster.core.nameserver import launch_nameserver
import ConfigSpace as CS
from tensorflow.keras.models import load_model

# Define the hyperparameter search space
def get_search_space():
    config_space = CS.ConfigurationSpace()

    # Hyperparameters to tune
    dense1_units = CS.IntegerHyperparameter('dense1_units', lower=128, upper=1024, log=True)
    dropout_rate = CS.UniformFloatHyperparameter('dropout_rate', lower=0.1, upper=0.5)
    learning_rate = CS.UniformFloatHyperparameter('learning_rate', lower=1e-5, upper=1e-2, log=True)

    config_space.add_hyperparameters([dense1_units, dropout_rate, learning_rate])
    return config_space


# Define the objective function to return validation accuracy
def objective_function(config):
    # Use the hyperparameters from the config object
    dense1_units = config['dense1_units']
    dropout_rate = config['dropout_rate']
    learning_rate = config['learning_rate']

    # Build the model with the current hyperparameters
    model = build_model(dense1_units=dense1_units, dropout_rate=dropout_rate, learning_rate=learning_rate, num_classes=num_classes)

    # Train the model on the training data
    history = model.fit(
        train_gen,
        epochs=3,  # Reduce for faster BOHB iterations
        validation_data=val_gen,
        verbose=0
    )

    # Return the validation accuracy for optimization
    val_accuracy = history.history['val_accuracy'][-1]
    return val_accuracy


# Start the Hyperparameter Optimization process with BOHB
def run_bohb():
    # Define the search space
    config_space = get_search_space()

    # Launch the nameserver and optimizer
    NS = launch_nameserver()

    # Initialize the optimizer
    optimizer = BOHB(configspace=config_space,
                     eta=3,  # Number of configurations to sample per iteration
                     min_points_in_model=4,  # Number of initial random evaluations
                     run_id="bohb_run",  # Name of the optimization run
                     nameserver='localhost',  # Name of the nameserver
                     nameserver_port=NS[1])  # Port of the nameserver

    # Start the optimization job with the objective function
    optimizer.run(background=True)

    # Run for a predefined number of iterations (e.g., 20)
    res = optimizer.get_results()

    # Retrieve the best configuration found by BOHB
    best_config = res.get_best_configuration()
    print(f"Best configuration: {best_config}")

    # Shutdown the optimizer and nameserver
    optimizer.shutdown()
    NS[0].shutdown()

    return best_config


# Run the Bayesian optimization
best_config = run_bohb()

# Use the best configuration found by BOHB to build the model
best_model = build_model(
    dense1_units=best_config['dense1_units'],
    dropout_rate=best_config['dropout_rate'],
    learning_rate=best_config['learning_rate'],
    num_classes=num_classes
)

# Train the model using the best configuration
history = best_model.fit(
    train_gen,
    epochs=25,  # Train for more epochs after optimization
    validation_data=val_gen,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Save the trained model
best_model.save('best_model_bohb.keras')

# Plot the training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy (BOHB Optimization)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# BOHB configuration#

import Pyro4
import ConfigSpace as CS
import numpy as np
from hpbandster.core.worker import Worker
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import seaborn as sns
import matplotlib.pyplot as plt
import socket  # For TCP connection

# Configure Pyro4 to use the remote nameserver (ngrok)
Pyro4.config.NS_HOST = "8.tcp.ngrok.io"  # Remote ngrok-hosted nameserver
Pyro4.config.NS_PORT = 14639  # The ngrok port for the Pyro4 nameserver

# Debugging output to verify Pyro4 config
print(f"Pyro4 NS_HOST: {Pyro4.config.NS_HOST}")
print(f"Pyro4 NS_PORT: {Pyro4.config.NS_PORT}")


# Custom Data Generator to handle both datasets (colon and lung)
class FusionDataGenerator(Sequence):
    def __init__(self, colon_gen, lung_gen, batch_size, steps_per_epoch, num_classes=5, **kwargs):
        super().__init__(**kwargs)
        self.colon_gen = colon_gen
        self.lung_gen = lung_gen
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.num_classes = num_classes

        self.current_index = 0

    def __len__(self):
        return self.steps_per_epoch

    def __next__(self):
        data = self.__getitem__(self.current_index)
        self.current_index += 1
        return data

    def __iter__(self):
        self.current_index = 0
        return self

    def _reset_iterators(self):
        self.colon_iterator = iter(self.colon_gen)
        self.lung_iterator = iter(self.lung_gen)
        self.current_index = 0

    def __getitem__(self, index):
        try:
            colon_batch = next(self.colon_iterator)
            lung_batch = next(self.lung_iterator)

            colon_images, colon_labels = colon_batch
            lung_images, lung_labels = lung_batch

            lung_labels = lung_labels + 2  # Adjust labels for lung data

            images = np.concatenate([colon_images, lung_images], axis=0)
            labels = np.concatenate([colon_labels, lung_labels], axis=0)

            return (images, labels)

        except StopIteration:
            self._reset_iterators()
            return self.__getitem__(index)


# Data generator functions for Colon and Lung datasets
colon_dataset_dir = '/content/Project_Sample/colon'
lung_dataset_dir = '/content/Project_Sample/lung'

def create_data_generator_colon(dataset_dir, batch_size):
    colon_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    colon_train_gen = colon_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['colon_aca', 'colon_n'],
        shuffle=True
    )
    return colon_train_gen

def create_data_generator_lung(dataset_dir, batch_size):
    lung_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.0,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest')

    lung_train_gen = lung_datagen.flow_from_directory(
        dataset_dir,
        target_size=(64, 64),
        batch_size=batch_size,
        class_mode='sparse',
        classes=['lung_aca', 'lung_n', 'lung_scc'],
        shuffle=True
    )
    return lung_train_gen

# Call the generators
colon_train_gen = create_data_generator_colon(colon_dataset_dir, batch_size=8)
lung_train_gen = create_data_generator_lung(lung_dataset_dir, batch_size=8)


# Transfer Learning with Pretrained Model (e.g., VGG16)
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

def build_cnn_model(input_shape=(64, 64, 3), learning_rate=0.0003176, num_classes=5):
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = True  # Unfreeze the entire base model

    # Fine-tune from a certain layer
    fine_tune_at = 15  # Start fine-tuning after this layer
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False

    model = models.Sequential()
    model.add(base_model)
    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout layer for regularization
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


# Learning Rate Scheduler
def lr_scheduler(epoch, lr):
    if epoch < 5:
        return lr
    else:
        return lr * 0.9  # Decay the learning rate after 5 epochs

# Define hyperparameter space for optimization
def define_hyperparameter_space():
    config_space = CS.ConfigurationSpace()
    learning_rate = CS.CategoricalHyperparameter('learning_rate', [0.0001, 0.001, 0.01])
    dropout_rate = CS.UniformFloatHyperparameter('dropout_rate', 0.0, 0.5)
    depth = CS.UniformIntegerHyperparameter('depth', lower=10, upper=100)
    iterations = CS.UniformIntegerHyperparameter('iterations', lower=100, upper=2000)
    lr_catboost = CS.UniformFloatHyperparameter('lr_catboost', lower=0.01, upper=0.2)

    config_space.add([learning_rate, dropout_rate, depth, iterations, lr_catboost])
    return config_space


# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


# Objective function for BOHB optimization
class CNNCatBoostWorker(Worker):
    def compute(self, config, budget, *args, **kwargs):
        # Extract hyperparameters
        learning_rate = config['learning_rate']
        dropout_rate = config['dropout_rate']
        depth = config['depth']
        iterations = config['iterations']
        lr_catboost = config['lr_catboost']

        # Create data generators
        colon_train_gen = create_data_generator_colon(colon_dataset_dir, batch_size=8)
        lung_train_gen = create_data_generator_lung(lung_dataset_dir, batch_size=8)

        # Create FusionDataGenerator
        train_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size=8, steps_per_epoch=5, num_classes=5)
        val_gen = FusionDataGenerator(colon_train_gen, lung_train_gen, batch_size=8, steps_per_epoch=5, num_classes=5)

        # Create CNN model
        cnn_model = build_cnn_model(input_shape=(64, 64, 3), learning_rate=learning_rate, num_classes=5)

        # EarlyStopping and Checkpoints for training
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)
        lr_scheduler_callback = LearningRateScheduler(lr_scheduler)

        # Train CNN model with early stopping, checkpoints, and learning rate scheduling
        cnn_model.fit(train_gen, epochs=20, validation_data=val_gen,
                      callbacks=[early_stopping, checkpoint, lr_scheduler_callback])

        # Extract features from CNN for training CatBoost
        train_features, train_labels = extract_features_from_cnn(cnn_model, train_gen, steps=5)
        val_features, val_labels = extract_features_from_cnn(cnn_model, val_gen, steps=5)

        # Train CatBoost model
        model_catboost = CatBoostClassifier(
            iterations=iterations,
            learning_rate=lr_catboost,
            depth=depth,
            loss_function='MultiClass'
        )
        model_catboost.fit(train_features, train_labels, eval_set=(val_features, val_labels), verbose=200)

        # Evaluate performance
        val_predictions = model_catboost.predict(val_features)
        accuracy = accuracy_score(val_labels, val_predictions)

        cm = confusion_matrix(val_labels, val_predictions)
        class_names = ['Colon_aca', 'Colon_n', 'Lung_aca', 'Lung_n', 'Lung_scc']
        plot_confusion_matrix(cm, class_names)

        # Return result
        return {'loss': -accuracy, 'status': 'ok'}


# Example usage:
if __name__ == "__main__":
    worker = CNNCatBoostWorker(run_id="worker_1")
    config_space = define_hyperparameter_space()

    config = {'learning_rate': 0.001, 'dropout_rate': 0.2, 'depth': 6, 'iterations': 1000, 'lr_catboost': 0.1}
    result = worker.compute(config, budget=1)
    print(result)
